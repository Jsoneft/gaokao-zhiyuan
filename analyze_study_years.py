import pandas as pd
import numpy as np

def analyze_excel_structure():
    """æ·±å…¥åˆ†æExcelæ–‡ä»¶çš„ç»“æ„ï¼Œå¤„ç†å¤æ‚çš„å¤´éƒ¨"""
    
    excel_file = "æœ€æ–°æœ€æ–°21-24å„çœæœ¬ç§‘ä¸“ä¸šåˆ† 1.xlsx"
    
    try:
        # é¦–å…ˆæŸ¥çœ‹åŸå§‹æ•°æ®çš„å‰å‡ è¡Œï¼Œä¸è·³è¿‡ä»»ä½•è¡Œ
        print("=== æŸ¥çœ‹Excelæ–‡ä»¶çš„åŸå§‹ç»“æ„ ===")
        raw_df = pd.read_excel(excel_file, header=None, nrows=10)
        print("å‰10è¡ŒåŸå§‹æ•°æ®:")
        for i in range(min(10, len(raw_df))):
            print(f"ç¬¬{i+1}è¡Œ: {list(raw_df.iloc[i].values)}")
        
        # å°è¯•ä¸åŒçš„headerè®¾ç½®
        print("\n=== å°è¯•ä½¿ç”¨ç¬¬2è¡Œä½œä¸ºheader ===")
        df_header1 = pd.read_excel(excel_file, header=1, nrows=5)
        print(f"åˆ—æ•°: {len(df_header1.columns)}")
        print("åˆ—å:")
        for i, col in enumerate(df_header1.columns):
            print(f"{i+1}. {col}")
        
        print("\nå‰5è¡Œæ•°æ®:")
        print(df_header1.head())
        
        # æŸ¥æ‰¾åŒ…å«"å­¦åˆ¶"ã€"å¹´åˆ¶"ç­‰å…³é”®è¯çš„åˆ—
        print("\n=== æŸ¥æ‰¾å­¦åˆ¶ç›¸å…³åˆ— ===")
        study_cols = []
        for i, col in enumerate(df_header1.columns):
            col_str = str(col).lower()
            if any(keyword in col_str for keyword in ['å­¦åˆ¶', 'å¹´åˆ¶', 'study', 'year', 'å­¦åˆ¶å¹´é™', 'ä¿®ä¸šå¹´é™']):
                study_cols.append((i, col))
        
        if study_cols:
            print(f"æ‰¾åˆ°å­¦åˆ¶ç›¸å…³åˆ—: {study_cols}")
        else:
            print("åœ¨åˆ—åä¸­æœªæ‰¾åˆ°æ˜æ˜¾çš„å­¦åˆ¶ç›¸å…³åˆ—")
            # æ£€æŸ¥æ•°æ®å†…å®¹
            print("\næ£€æŸ¥å‰100è¡Œæ•°æ®ä¸­æ˜¯å¦åŒ…å«å­¦åˆ¶ä¿¡æ¯...")
            for col_idx, col_name in enumerate(df_header1.columns):
                sample_values = df_header1[col_name].dropna().astype(str).head(20).tolist()
                # æ£€æŸ¥å€¼ä¸­æ˜¯å¦åŒ…å«å­¦åˆ¶ç›¸å…³ä¿¡æ¯
                has_study_info = any(
                    any(keyword in str(val).lower() for keyword in ['å¹´', 'å­¦åˆ¶', 'year'])
                    for val in sample_values
                    if str(val) not in ['nan', '']
                )
                if has_study_info:
                    print(f"åˆ— {col_idx+1} ({col_name}) å¯èƒ½åŒ…å«å­¦åˆ¶ä¿¡æ¯:")
                    print(f"æ ·æœ¬å€¼: {sample_values}")
        
        return df_header1
        
    except Exception as e:
        print(f"åˆ†æExcelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def check_id_and_study_years():
    """æ£€æŸ¥IDå¯¹åº”å…³ç³»å’Œå­¦åˆ¶ä¿¡æ¯"""
    
    try:
        # ä½¿ç”¨æ­£ç¡®çš„headerè¯»å–æ–°æ–‡ä»¶
        new_df = pd.read_excel("æœ€æ–°æœ€æ–°21-24å„çœæœ¬ç§‘ä¸“ä¸šåˆ† 1.xlsx", header=1)
        
        print(f"\n=== æ–°æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ ===")
        print(f"è¡Œæ•°: {len(new_df)}")
        print(f"åˆ—æ•°: {len(new_df.columns)}")
        
        # æŸ¥æ‰¾IDåˆ—
        id_col = None
        for col in new_df.columns:
            if 'id' in str(col).lower() or str(col).strip() == 'id':
                id_col = col
                break
        
        if id_col is None:
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æ˜¯ID
            first_col = new_df.columns[0]
            first_col_values = new_df[first_col].dropna()
            if len(first_col_values) > 0 and str(first_col_values.iloc[0]).isdigit():
                id_col = first_col
                print(f"å‡è®¾ç¬¬ä¸€åˆ— '{first_col}' æ˜¯IDåˆ—")
        
        if id_col:
            print(f"æ‰¾åˆ°IDåˆ—: {id_col}")
            print(f"IDå”¯ä¸€å€¼æ•°é‡: {new_df[id_col].nunique()}")
            print(f"IDèŒƒå›´: {new_df[id_col].min()} - {new_df[id_col].max()}")
            
            # æ£€æŸ¥ä¸åŸæ–‡ä»¶çš„å¯¹åº”å…³ç³»
            original_df = pd.read_excel("21-24å„çœä»½å½•å–æ•°æ®(å«ä¸“ä¸šç»„ä»£ç ).xlsx")
            if 'id' in original_df.columns:
                new_ids = set(new_df[id_col].dropna().astype(int))
                original_ids = set(original_df['id'].dropna().astype(int))
                
                common_ids = new_ids.intersection(original_ids)
                coverage = len(common_ids) / len(original_ids) * 100
                
                print(f"\n=== IDå¯¹åº”å…³ç³» ===")
                print(f"æ–°æ–‡ä»¶IDæ•°: {len(new_ids)}")
                print(f"åŸæ–‡ä»¶IDæ•°: {len(original_ids)}")
                print(f"å…±åŒIDæ•°: {len(common_ids)}")
                print(f"è¦†ç›–ç‡: {coverage:.2f}%")
                
                if coverage > 80:
                    print("âœ… IDè¦†ç›–ç‡è‰¯å¥½ï¼Œå¯ä»¥è¿›è¡Œæ•°æ®åˆå¹¶")
                else:
                    print("âŒ IDè¦†ç›–ç‡ä¸è¶³ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        
        # è¯¦ç»†æ£€æŸ¥æ‰€æœ‰åˆ—ï¼Œå¯»æ‰¾å­¦åˆ¶ä¿¡æ¯
        print(f"\n=== è¯¦ç»†åˆ—åˆ†æ ===")
        for i, col in enumerate(new_df.columns):
            print(f"\nåˆ— {i+1}: {col}")
            non_null_count = new_df[col].notna().sum()
            unique_count = new_df[col].nunique()
            print(f"  éç©ºå€¼æ•°é‡: {non_null_count}")
            print(f"  å”¯ä¸€å€¼æ•°é‡: {unique_count}")
            
            if non_null_count > 0:
                sample_values = new_df[col].dropna().head(10).tolist()
                print(f"  æ ·æœ¬å€¼: {sample_values}")
                
                # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯å­¦åˆ¶ä¿¡æ¯
                if unique_count < 20 and non_null_count > 1000:  # å­¦åˆ¶é€šå¸¸æ˜¯å°‘æ•°å‡ ä¸ªå€¼
                    all_values = new_df[col].dropna().unique()
                    print(f"  æ‰€æœ‰å”¯ä¸€å€¼: {all_values}")
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—å¹´åˆ¶ä¿¡æ¯
                    year_pattern_found = any(
                        str(val) in ['1', '2', '3', '4', '5', '6', '7', '8'] or
                        'å¹´' in str(val) or
                        'year' in str(val).lower()
                        for val in all_values
                    )
                    
                    if year_pattern_found:
                        print(f"  ğŸ¯ å¯èƒ½æ˜¯å­¦åˆ¶åˆ—ï¼")
        
        return new_df
        
    except Exception as e:
        print(f"æ£€æŸ¥IDå’Œå­¦åˆ¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None

def create_mapping_analysis():
    """åˆ›å»ºè¯¦ç»†çš„æ˜ å°„åˆ†æ"""
    
    print("\n=== åˆ›å»ºè¯¦ç»†æ˜ å°„åˆ†æ ===")
    
    try:
        # è¯»å–ä¸¤ä¸ªæ–‡ä»¶
        new_df = pd.read_excel("æœ€æ–°æœ€æ–°21-24å„çœæœ¬ç§‘ä¸“ä¸šåˆ† 1.xlsx", header=1)
        original_df = pd.read_excel("21-24å„çœä»½å½•å–æ•°æ®(å«ä¸“ä¸šç»„ä»£ç ).xlsx")
        
        # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
        with open("study_years_analysis.txt", "w", encoding="utf-8") as f:
            f.write("å­¦åˆ¶ä¿¡æ¯åˆ†ææŠ¥å‘Š\n")
            f.write("="*50 + "\n\n")
            
            f.write(f"æ–°æ–‡ä»¶è¡Œæ•°: {len(new_df)}\n")
            f.write(f"æ–°æ–‡ä»¶åˆ—æ•°: {len(new_df.columns)}\n")
            f.write(f"åŸæ–‡ä»¶è¡Œæ•°: {len(original_df)}\n")
            f.write(f"åŸæ–‡ä»¶åˆ—æ•°: {len(original_df.columns)}\n\n")
            
            f.write("æ–°æ–‡ä»¶åˆ—ä¿¡æ¯:\n")
            for i, col in enumerate(new_df.columns):
                non_null = new_df[col].notna().sum()
                unique = new_df[col].nunique()
                f.write(f"{i+1:2d}. {col:30s} éç©º:{non_null:8d} å”¯ä¸€:{unique:6d}\n")
            
            # æŸ¥æ‰¾æœ€å¯èƒ½çš„å­¦åˆ¶åˆ—
            potential_study_cols = []
            for i, col in enumerate(new_df.columns):
                unique_count = new_df[col].nunique()
                non_null_count = new_df[col].notna().sum()
                
                if 2 <= unique_count <= 15 and non_null_count > 10000:
                    unique_vals = new_df[col].dropna().unique()
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´åˆ¶ä¿¡æ¯
                    has_year_info = any(
                        str(val).strip() in ['1', '2', '3', '4', '5', '6', '7', '8'] or
                        'å¹´' in str(val) or
                        'year' in str(val).lower()
                        for val in unique_vals
                    )
                    
                    if has_year_info:
                        potential_study_cols.append((i, col, unique_vals))
            
            f.write(f"\næ½œåœ¨çš„å­¦åˆ¶åˆ—:\n")
            for idx, col, vals in potential_study_cols:
                f.write(f"{idx+1}. {col}: {vals}\n")
        
        print("è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ° study_years_analysis.txt")
        
        if potential_study_cols:
            print(f"\næ‰¾åˆ° {len(potential_study_cols)} ä¸ªæ½œåœ¨çš„å­¦åˆ¶åˆ—:")
            for idx, col, vals in potential_study_cols:
                print(f"  åˆ— {idx+1}: {col}")
                print(f"    å”¯ä¸€å€¼: {vals}")
        
        return potential_study_cols
        
    except Exception as e:
        print(f"åˆ›å»ºæ˜ å°„åˆ†ææ—¶å‡ºé”™: {e}")
        return []

if __name__ == "__main__":
    print("å¼€å§‹åˆ†ææ–°Excelæ–‡ä»¶çš„å­¦åˆ¶ä¿¡æ¯...")
    
    # æ­¥éª¤1: åˆ†æExcelç»“æ„
    df = analyze_excel_structure()
    
    if df is not None:
        # æ­¥éª¤2: æ£€æŸ¥IDå¯¹åº”å…³ç³»å’Œå­¦åˆ¶ä¿¡æ¯
        new_df = check_id_and_study_years()
        
        # æ­¥éª¤3: åˆ›å»ºè¯¦ç»†æ˜ å°„åˆ†æ
        potential_cols = create_mapping_analysis()
        
        if potential_cols:
            print(f"\n=== ç»“è®º ===")
            print(f"âœ… å‘ç° {len(potential_cols)} ä¸ªå¯èƒ½çš„å­¦åˆ¶åˆ—")
            print("å»ºè®®è¿›è¡Œä¸‹ä¸€æ­¥çš„æ•°æ®å¯¼å…¥æµ‹è¯•")
        else:
            print(f"\n=== ç»“è®º ===")
            print("âŒ æœªæ‰¾åˆ°æ˜ç¡®çš„å­¦åˆ¶åˆ—ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ") 